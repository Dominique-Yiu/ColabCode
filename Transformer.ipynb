{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMxU1/ovZwDryNitMwuYB6r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dominique-Yiu/ColabCode/blob/main/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Realization\n",
        "**Transformer Architecture** \\\n",
        "1. Encoder\n",
        "- Input words embedding\n",
        "  - Turn the sparse one-hot vector into the dense contiguouly vector by FFN without bias.\n",
        "- Position encoding\n",
        "- Multi-head self-attention\n",
        "- Feed-forword network\n",
        "2. Decoder\n",
        "- Output words embedding\n",
        "- Masked multi-head self-attention\n",
        "- Multi-head cross-attention\n",
        "- Feed-forword network\n",
        "- Softmax"
      ],
      "metadata": {
        "id": "_U-F1ziytKW_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYbEqVzHtIHm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "# the size of the word sheet\n",
        "max_num_src_words = 8\n",
        "max_num_tgt_words = 8\n",
        "# \n",
        "model_dim = 8\n",
        "# the max length of sequence\n",
        "max_src_seq_len = 5\n",
        "max_tgt_seq_len = 5\n",
        "# 位置索引最大值\n",
        "max_position_len = 5\n",
        "# generate the sequence length randomly, its size is fixed\n",
        "# src_len = torch.randint(2, 5, (batch_size,))\n",
        "# tgt_len = torch.randint(2, 5, (batch_size,))\n",
        "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
        "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
        "\n",
        "# generate the src/tgt sentence, and pad this sentence with default value '0'\n",
        "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max(src_len) - L)), 0) \\\n",
        "           for L in src_len])\n",
        "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max(tgt_len) - L)), 0) \\\n",
        "           for L in tgt_len])\n",
        "\n",
        "\"\"\"Word Embedding\"\"\"\n",
        "# 构造 Word Embedding\n",
        "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim)\n",
        "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)\n",
        "src_embedding = src_embedding_table(src_seq)\n",
        "tgt_embedding = tgt_embedding_table(tgt_seq)\n",
        "\n",
        "print(src_embedding_table.weight)\n",
        "print(src_seq)\n",
        "print(src_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_gW7RHHwOEr",
        "outputId": "610e1874-30c3-4dff-d88a-0988da9063a2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-1.3421, -0.5922,  0.7105, -0.4137, -0.7792,  0.8271, -1.1737,  1.1708],\n",
            "        [-0.7755, -0.8535, -0.8674,  1.2121, -0.1808,  1.3193,  0.4921,  0.3922],\n",
            "        [-0.3492,  0.6076,  0.5111,  1.8655, -0.8650, -0.2841,  0.5962,  0.1457],\n",
            "        [ 0.7635, -0.8373, -2.0204, -0.5166,  0.4622, -1.5357,  0.7542,  0.3448],\n",
            "        [-0.6157, -0.9285,  1.5628, -0.3538,  0.3983,  0.9700,  1.7446, -0.3886],\n",
            "        [-0.3686,  1.4301, -0.6616, -1.0094, -0.7734, -0.6186, -1.6067, -0.7960],\n",
            "        [-2.1911,  0.5905,  0.1929,  0.6911, -0.0636,  0.0418,  0.7898, -0.2072],\n",
            "        [-1.5851,  1.5669, -0.3050,  0.9676,  0.7270, -0.2039,  0.9162,  0.7299],\n",
            "        [ 0.3286, -0.9448,  1.1110,  1.0238, -1.2310,  0.6511, -1.2549,  0.1557]],\n",
            "       requires_grad=True)\n",
            "tensor([[7, 4, 0, 0],\n",
            "        [2, 3, 7, 1]])\n",
            "tensor([[[-1.5851,  1.5669, -0.3050,  0.9676,  0.7270, -0.2039,  0.9162,\n",
            "           0.7299],\n",
            "         [-0.6157, -0.9285,  1.5628, -0.3538,  0.3983,  0.9700,  1.7446,\n",
            "          -0.3886],\n",
            "         [-1.3421, -0.5922,  0.7105, -0.4137, -0.7792,  0.8271, -1.1737,\n",
            "           1.1708],\n",
            "         [-1.3421, -0.5922,  0.7105, -0.4137, -0.7792,  0.8271, -1.1737,\n",
            "           1.1708]],\n",
            "\n",
            "        [[-0.3492,  0.6076,  0.5111,  1.8655, -0.8650, -0.2841,  0.5962,\n",
            "           0.1457],\n",
            "         [ 0.7635, -0.8373, -2.0204, -0.5166,  0.4622, -1.5357,  0.7542,\n",
            "           0.3448],\n",
            "         [-1.5851,  1.5669, -0.3050,  0.9676,  0.7270, -0.2039,  0.9162,\n",
            "           0.7299],\n",
            "         [-0.7755, -0.8535, -0.8674,  1.2121, -0.1808,  1.3193,  0.4921,\n",
            "           0.3922]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Position Embedding\"\"\"\n",
        "# 构造Position Embedding\n",
        "pos_mat  =  torch.arange(max_position_len).reshape((-1, 1))\n",
        "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1)) / model_dim)\n",
        "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
        "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
        "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
        "\n",
        "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
        "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad = False)\n",
        "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
        "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
        "\n",
        "src_pos_embedding = pe_embedding(src_pos)\n",
        "tgt_pos_embedding = pe_embedding(tgt_pos)\n",
        "print(src_pos_embedding)\n",
        "print(tgt_pos_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryUZJMKGxHOW",
        "outputId": "7e26e65b-2f81-4716-a283-969f2babae33"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n",
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]],\n",
            "\n",
            "        [[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
            "           1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.4030e-01,  9.9833e-02,  9.9500e-01,  9.9998e-03,\n",
            "           9.9995e-01,  1.0000e-03,  1.0000e+00],\n",
            "         [ 9.0930e-01, -4.1615e-01,  1.9867e-01,  9.8007e-01,  1.9999e-02,\n",
            "           9.9980e-01,  2.0000e-03,  1.0000e+00],\n",
            "         [ 1.4112e-01, -9.8999e-01,  2.9552e-01,  9.5534e-01,  2.9995e-02,\n",
            "           9.9955e-01,  3.0000e-03,  1.0000e+00]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V$"
      ],
      "metadata": {
        "id": "aAJicY_pMkJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\"\"\"Encoder: Self-Attention Mask\"\"\"\n",
        "# 构造encoder的self-attention mask\n",
        "# mask的shape: [batch_size, max_src_len, max_src_len]，数值为1/-inf\n",
        "valid_encoder_pos = torch.unsqueeze(torch.cat([torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len) - L)), 0) for L in src_len]), 2)\n",
        "valid_encoder_pos_matrix = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(1, 2))\n",
        "invalid_encoder_pos_matrix = 1 - valid_encoder_pos_matrix\n",
        "mask_encoder_self_attention = invalid_encoder_pos_matrix.to(torch.bool)\n",
        "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
        "masked_score = score.masked_fill(mask_encoder_self_attention, -1e9)\n",
        "prob = F.softmax(masked_score, -1)\n",
        "\n",
        "print(score)\n",
        "print(masked_score)\n",
        "print(prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "towG0uO6-rPP",
        "outputId": "672ee7ef-18c6-4318-ea6d-3270c4ea0b6d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 2.3577, -0.2677,  0.8554, -1.4181],\n",
            "         [-0.5159,  0.1013,  1.0361, -0.9863],\n",
            "         [-0.6121,  1.4148, -1.6303, -1.5475],\n",
            "         [-0.0495, -0.0552,  0.1952, -1.0923]],\n",
            "\n",
            "        [[-0.2355, -1.3529,  0.3448,  0.3238],\n",
            "         [ 0.0567,  0.5248, -1.5022, -0.0481],\n",
            "         [ 1.2487, -0.4582, -1.1756, -0.1060],\n",
            "         [ 0.7508,  2.8218, -0.1943, -0.6140]]])\n",
            "tensor([[[ 2.3577e+00, -2.6770e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [-5.1594e-01,  1.0129e-01, -1.0000e+09, -1.0000e+09],\n",
            "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
            "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]],\n",
            "\n",
            "        [[-2.3549e-01, -1.3529e+00,  3.4482e-01,  3.2380e-01],\n",
            "         [ 5.6668e-02,  5.2479e-01, -1.5022e+00, -4.8116e-02],\n",
            "         [ 1.2487e+00, -4.5824e-01, -1.1756e+00, -1.0598e-01],\n",
            "         [ 7.5077e-01,  2.8218e+00, -1.9434e-01, -6.1401e-01]]])\n",
            "tensor([[[0.9325, 0.0675, 0.0000, 0.0000],\n",
            "         [0.3504, 0.6496, 0.0000, 0.0000],\n",
            "         [0.2500, 0.2500, 0.2500, 0.2500],\n",
            "         [0.2500, 0.2500, 0.2500, 0.2500]],\n",
            "\n",
            "        [[0.2056, 0.0673, 0.3674, 0.3597],\n",
            "         [0.2697, 0.4307, 0.0567, 0.2429],\n",
            "         [0.6545, 0.1187, 0.0579, 0.1689],\n",
            "         [0.1044, 0.8283, 0.0406, 0.0267]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上主要实现了词向量，位置编码，编码器子注意力的掩码"
      ],
      "metadata": {
        "id": "B0NgCEIZTo3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9uQ1UWZqNw7n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}